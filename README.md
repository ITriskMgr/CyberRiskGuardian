# CyberRiskGuardian
This project represents a proof-of-concept (PoC) for a Retrieval-Augmented Generation (RAG)-enhanced local LLM application. It is designed to provide professionals with a robust, secure, and customizable solution for querying locally stored knowledge repositories. By combining the retrieval capabilities of a vector database with the language generation power of a Large Language Model (LLM), this project allows users to interact with their own datasets efficiently, enabling the generation of accurate, contextually relevant responses.
At its core, the system integrates several components to create an end-to-end RAG pipeline. The pipeline processes and indexes a collection of documents into a vector database using embeddings generated by the selected LLM. These embeddings enable the system to perform similarity-based searches, ensuring that queries retrieve the most relevant data. Once retrieved, the system uses the LLM to synthesize the retrieved content with its pre-trained knowledge, generating responses tailored to the user's needs.
The application was built in a Python 3.10 environment and runs on PyCharm Professional Edition, leveraging the Ollama LLM platform for language generation and the Chroma vector database for information retrieval. It operates on a datastore of over 6,000 documents, including PDFs and text files, compiled over years of research and professional work in cybersecurity and business technology management.
The workflow begins with the ingestion of documents, which are then split into manageable chunks using a text-splitting strategy that ensures contextual continuity. These chunks are embedded and stored in the vector database, enabling efficient retrieval. The pipeline also includes a GUI built with tkinter, allowing users to interact with the system and submit queries in real time.
